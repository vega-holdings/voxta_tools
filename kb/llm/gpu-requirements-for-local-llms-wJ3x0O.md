---
id: wJ3x0ONM0dBXIASi_DOIq
slug: gpu-requirements-for-local-llms-wJ3x0O
type: reference
title: Hardware Requirements for Local LLMs
topics:
  - llm
  - performance
  - setup
keywords:
  - GPU
  - RTX
  - hardware
  - requirements
  - speed
  - "3090"
  - 3090ti
  - PSU
  - power supply
  - local LLM
confidence: 0.6
authority: 0.2
contributor: futurestorm
sources: []
created: 2025-12-22T23:38:49.098Z
updated: 2025-12-23T14:01:31.466Z
---

# Hardware Requirements for Local LLMs

> NVIDIA RTX GPUs are recommended for running local LLMs to ensure speed, with the RTX 3090/Ti being a cost-effective choice requiring at least an 850W PSU.

## Answer

When running Large Language Models (LLMs) locally, a powerful GPU is required to avoid slow generation speeds. **NVIDIA RTX series cards** are specifically recommended to ensure adequate performance.

For a cost-effective setup, the **RTX 3090 / 3090 Ti** is often recommended compared to newer, more expensive series cards.

**Power Requirement:** Ensure your system has at least an **850W power supply (PSU)** to support the power draw of these high-performance GPUs.

---

*Extracted from Discord. Primary contributor: futurestorm*