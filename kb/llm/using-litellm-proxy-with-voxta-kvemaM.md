---
id: kvemaMemVlrGnQ9mEaEnJ
slug: using-litellm-proxy-with-voxta-kvemaM
type: reference
title: Using LiteLLM Proxy with Voxta
topics:
  - llm
  - configuration
keywords:
  - litellm
  - proxy
  - api
  - providers
confidence: 0.7
authority: 0.2
contributor: daisukearamaki
sources: []
created: 2025-12-23T00:07:56.216Z
updated: 2025-12-23T00:07:56.216Z
---

# Using LiteLLM Proxy with Voxta

> Reference to LiteLLM for proxying additional LLM providers.

## Answer

LiteLLM can be used as a proxy to connect various LLM providers to Voxta. This is useful for accessing models not natively supported or for managing API connections via an OpenAI-compatible endpoint.

Documentation: [LiteLLM Simple Proxy](https://docs.litellm.ai/docs/simple_proxy)

---

*Extracted from Discord. Primary contributor: daisukearamaki*