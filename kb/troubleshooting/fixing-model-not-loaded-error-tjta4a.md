---
id: tjta4af1lJLv0MYuwnDz3
slug: fixing-model-not-loaded-error-tjta4a
type: troubleshooting
title: Fixing 'Model Not Loaded' Error
topics:
  - troubleshooting
  - llm
  - configuration
keywords:
  - model not loaded
  - error
  - F2
  - logs
  - debug
confidence: 0.8
authority: 0.2
contributor: sraura
sources:
  - messageId: "1348501861396910170"
    author: sraura
    timestamp: 2025-12-22T21:22:45.035Z
    url: ""
created: 2025-12-22T21:22:45.035Z
updated: 2025-12-22T21:22:45.035Z
---

# Fixing 'Model Not Loaded' Error

> Resolve 'model not loaded' errors by verifying the model configuration and checking server logs (F2).

## Answer

The 'model not loaded' error typically indicates that the LLM is either not specified in the model loader configuration or failed to initialize.

**Troubleshooting Steps:**
1. Check the model loader configuration to ensure a model is selected.
2. Press **F2** in the Voxta Server window to view the logs. This will provide specific error details regarding why the model failed to load.

---

*Extracted from Discord. Primary contributor: sraura*